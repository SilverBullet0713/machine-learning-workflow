{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e27603-851f-4cb0-bd36-3e49e232f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import shap\n",
    "import pickle\n",
    "import csv\n",
    "import ast\n",
    "import warnings\n",
    "from sklearn.model_selection import ShuffleSplit, KFold\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 配置与全局变量\n",
    "# 文件路径配置\n",
    "DATA_OLD_PATH = r\"data/data_per_0401.xlsx\"\n",
    "DATA_FP_PATH = r\"data/fp_file.xlsx\"\n",
    "TRAIN_DATA_PATH = 'data/per_train_data.csv'\n",
    "TEST_DATA_PATH = 'data/per_test.csv'\n",
    "MODEL_SELECTION_RESULTS_PATH = r'results/per_model_selection.xlsx'\n",
    "HYPEROPT_LOG_PATH = 'results/hyperopt_log.csv'\n",
    "TRIALS_PATH = r\"results/hyperopt_trials.p\"\n",
    "SAVED_MODEL_PATH = 'saved_models/per_model.model'\n",
    "\n",
    "# 特征列定义\n",
    "FP_COLUMNS = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "NUMERIC_FEATURES = []\n",
    "TARGET_COLUMN = 'Permeability'\n",
    "\n",
    "# 全局迭代计数器 (用于超参数优化)\n",
    "ITERATION_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a755d-ea4d-4d1a-bc74-7c2c460ef848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.数据预处理 (Data Preprocessing)\n",
    "def run_data_preprocessing():\n",
    "    # 读取数据\n",
    "    data_old = pd.read_excel(DATA_OLD_PATH)\n",
    "    data_fp = pd.read_excel(DATA_FP_PATH)\n",
    "\n",
    "    # 动态定义全局特征列\n",
    "    global FP_COLUMNS, CATEGORICAL_FEATURES, NUMERIC_FEATURES\n",
    "    FP_COLUMNS = list(data_fp.columns)\n",
    "    data = pd.concat([data_old, data_fp], axis=1)\n",
    "\n",
    "    CATEGORICAL_FEATURES = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    columns_to_exclude = [TARGET_COLUMN] + FP_COLUMNS\n",
    "    NUMERIC_FEATURES = [col for col in data.select_dtypes(include=['int64', 'float64']).columns if\n",
    "                        col not in columns_to_exclude]\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2, random_state=10)\n",
    "    train_index, test_index = next(sss.split(data))\n",
    "    train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "    test_data = data.iloc[test_index].reset_index(drop=True)\n",
    "\n",
    "    # 对数值特征使用MICE插补法 (IterativeImputer)\n",
    "    cols_positive = ['MWCO', 'Soaking Time', 'Filtration Area']\n",
    "    imputer = IterativeImputer(random_state=10)\n",
    "\n",
    "    # 为保证插补值为正，先对数变换，插补后再指数变换恢复\n",
    "    for col in cols_positive:\n",
    "        train_data[col] = np.log(train_data[col].clip(lower=1e-6))\n",
    "        test_data[col] = np.log(test_data[col].clip(lower=1e-6))\n",
    "\n",
    "    train_data[NUMERIC_FEATURES] = imputer.fit_transform(train_data[NUMERIC_FEATURES])\n",
    "    test_data[NUMERIC_FEATURES] = imputer.transform(test_data[NUMERIC_FEATURES])\n",
    "\n",
    "    for col in cols_positive:\n",
    "        train_data[col] = np.exp(train_data[col])\n",
    "        test_data[col] = np.exp(test_data[col])\n",
    "\n",
    "    # 保存处理后的数据\n",
    "    train_data.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "    test_data.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "    print(f\"预处理完成，训练数据已保存至: {TRAIN_DATA_PATH}\")\n",
    "    print(f\"预处理完成，测试数据已保存至: {TEST_DATA_PATH}\")\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "    # 对给定的数据集进行特征编码和缩放\n",
    "def preprocess_features(data_to_transform, data_to_fit_on, encoder, scaler):\n",
    "    # 数据划分\n",
    "    fp_df = data_to_transform[FP_COLUMNS].copy()\n",
    "    X_fit = data_to_fit_on.drop(columns=[TARGET_COLUMN] + FP_COLUMNS)\n",
    "    y_fit = data_to_fit_on[TARGET_COLUMN]\n",
    "\n",
    "    # 训练编码器和缩放器\n",
    "    X_fit_encoded = encoder.fit_transform(X_fit, y_fit)\n",
    "    scaler.fit(X_fit_encoded[NUMERIC_FEATURES])\n",
    "\n",
    "    # 应用于待转换数据\n",
    "    X_transform = data_to_transform.drop(columns=[TARGET_COLUMN] + FP_COLUMNS)\n",
    "    y_transform = data_to_transform[TARGET_COLUMN].values\n",
    "\n",
    "    X_transform_encoded = encoder.transform(X_transform)\n",
    "\n",
    "    # 分离编码后的类别和数值部分\n",
    "    cat_df = X_transform_encoded.drop(columns=NUMERIC_FEATURES)\n",
    "    num_array = scaler.transform(X_transform_encoded[NUMERIC_FEATURES])\n",
    "    num_df = pd.DataFrame(data=num_array, columns=NUMERIC_FEATURES, index=data_to_transform.index)\n",
    "\n",
    "    # 合并最终特征矩阵\n",
    "    final_X = pd.concat([fp_df, cat_df, num_df], axis=1)\n",
    "\n",
    "    return final_X, y_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbae10-15a9-466c-90ea-d852b005b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.模型筛选 (Model Selection)\n",
    "def run_model_selection():\n",
    "    # 通过5折交叉验证，系统地评估不同模型、编码器和缩放器的组合。\n",
    "    print(\"--- 开始执行: 模型筛选 ---\")\n",
    "    train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "    # 定义候选模型、编码器和缩放器\n",
    "    models = [\n",
    "        CatBoostRegressor(verbose=False, random_state=10),\n",
    "        XGBRegressor(random_state=10),\n",
    "        SVR(),\n",
    "        RandomForestRegressor(random_state=10),\n",
    "        GradientBoostingRegressor(random_state=10),\n",
    "        LinearRegression(),\n",
    "        DecisionTreeRegressor(random_state=10),\n",
    "        AdaBoostRegressor(random_state=10)\n",
    "    ]\n",
    "    encoders = [\n",
    "        ce.BackwardDifferenceEncoder(cols=CATEGORICAL_FEATURES), ce.BaseNEncoder(cols=CATEGORICAL_FEATURES),\n",
    "        ce.BinaryEncoder(cols=CATEGORICAL_FEATURES), ce.HelmertEncoder(cols=CATEGORICAL_FEATURES),\n",
    "        ce.JamesSteinEncoder(cols=CATEGORICAL_FEATURES), ce.OneHotEncoder(cols=CATEGORICAL_FEATURES),\n",
    "        ce.MEstimateEncoder(cols=CATEGORICAL_FEATURES), ce.SumEncoder(cols=CATEGORICAL_FEATURES)\n",
    "    ]\n",
    "    scalers = [StandardScaler(), MinMaxScaler(), MaxAbsScaler(), RobustScaler(), PowerTransformer()]\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for model in models:\n",
    "        for en in encoders:\n",
    "            for sc in scalers:\n",
    "                metrics = {'t_rmse': [], 't_r2': [], 't_mae': [], 'v_rmse': [], 'v_r2': [], 'v_mae': []}\n",
    "\n",
    "                for train_idx, val_idx in kf.split(train_data):\n",
    "                    train_fold = train_data.iloc[train_idx].reset_index(drop=True)\n",
    "                    val_fold = train_data.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "                    x_train_fold, y_train_fold = preprocess_features(train_fold, train_fold, en, sc)\n",
    "                    x_val_fold, y_val_fold = preprocess_features(val_fold, train_fold, en, sc)\n",
    "\n",
    "                    model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "                    y_train_pred = model.predict(x_train_fold)\n",
    "                    y_val_pred = model.predict(x_val_fold)\n",
    "\n",
    "                    metrics['t_rmse'].append(np.sqrt(mean_squared_error(y_train_fold, y_train_pred)))\n",
    "                    metrics['t_r2'].append(r2_score(y_train_fold, y_train_pred))\n",
    "                    metrics['t_mae'].append(mean_absolute_error(y_train_fold, y_train_pred))\n",
    "                    metrics['v_rmse'].append(np.sqrt(mean_squared_error(y_val_fold, y_val_pred)))\n",
    "                    metrics['v_r2'].append(r2_score(y_val_fold, y_val_pred))\n",
    "                    metrics['v_mae'].append(mean_absolute_error(y_val_fold, y_val_pred))\n",
    "\n",
    "                results_list.append({\n",
    "                    'train_rmse': np.mean(metrics['t_rmse']), 'test_rmse': np.mean(metrics['v_rmse']),\n",
    "                    'train_r2': np.mean(metrics['t_r2']), 'test_r2': np.mean(metrics['v_r2']),\n",
    "                    'train_mae': np.mean(metrics['t_mae']), 'test_mae': np.mean(metrics['v_mae']),\n",
    "                    'name': model.__class__.__name__, 'encoder': en.__class__.__name__, 'scaler': sc.__class__.__name__\n",
    "                })\n",
    "\n",
    "    results = pd.DataFrame(results_list)\n",
    "    results.sort_values('test_rmse', ascending=True, inplace=True)\n",
    "    results.to_excel(MODEL_SELECTION_RESULTS_PATH, index=False)\n",
    "\n",
    "    print(f\"模型筛选完成，结果已保存至: {MODEL_SELECTION_RESULTS_PATH}\")\n",
    "    print(\"表现最佳的前5个组合:\")\n",
    "    print(results.head(5))\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "    return results.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b368-d32d-4459-9e63-41c48a943635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.模型优化 (Model Optimization)\n",
    "def run_hyperparameter_optimization(best_encoder_name, best_scaler_name):\n",
    "    print(\"--- 开始执行: 模型超参数优化 ---\")\n",
    "    train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "    # 根据模型筛选结果，实例化最佳的编码器和缩放器\n",
    "    sc = PowerTransformer()\n",
    "    en = ce.BackwardDifferenceEncoder(cols=CATEGORICAL_FEATURES)\n",
    "\n",
    "    space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "        'depth': hp.quniform('depth', 3, 6, 1),\n",
    "        'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 3),\n",
    "        'iterations': hp.quniform('iterations', 600, 800, 10),\n",
    "        'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "        'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 10, 1),\n",
    "        'random_strength': hp.uniform('random_strength', 0, 5),\n",
    "        'bagging_temperature': hp.uniform('bagging_temperature', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        global ITERATION_COUNTER\n",
    "        ITERATION_COUNTER += 1\n",
    "\n",
    "        # 确保整数参数类型正确\n",
    "        for name in ['depth', 'iterations', 'min_data_in_leaf']:\n",
    "            params[name] = int(params[name])\n",
    "\n",
    "        model = CatBoostRegressor(**params, random_state=10, verbose=False)\n",
    "\n",
    "        val_rmses = []\n",
    "        for train_idx, val_idx in kf.split(train_data):\n",
    "            train_fold = train_data.iloc[train_idx].reset_index(drop=True)\n",
    "            val_fold = train_data.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "            x_train_fold, y_train_fold = preprocess_features(train_fold, train_fold, en, sc)\n",
    "            x_val_fold, y_val_fold = preprocess_features(val_fold, train_fold, en, sc)\n",
    "\n",
    "            model.fit(x_train_fold, y_train_fold)\n",
    "            y_val_pred = model.predict(x_val_fold)\n",
    "            val_rmses.append(np.sqrt(mean_squared_error(y_val_fold, y_val_pred)))\n",
    "\n",
    "        loss = np.mean(val_rmses)\n",
    "\n",
    "        # 记录日志\n",
    "        with open(HYPEROPT_LOG_PATH, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([loss, params, ITERATION_COUNTER])\n",
    "\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    # 初始化日志文件\n",
    "    with open(HYPEROPT_LOG_PATH, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['loss', 'params', 'iteration'])\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100,\n",
    "                trials=trials, rstate=np.random.default_rng(50))\n",
    "\n",
    "    # 保存Trials对象以便后续分析\n",
    "    with open(TRIALS_PATH, \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "\n",
    "    # 读取并排序结果\n",
    "    result = pd.read_csv(HYPEROPT_LOG_PATH)\n",
    "    result.sort_values('loss', ascending=True, inplace=True)\n",
    "    best_params_str = result.iloc[0]['params']\n",
    "    best_params = ast.literal_eval(best_params_str)\n",
    "\n",
    "    print(f\"超参数优化完成，日志已保存至: {HYPEROPT_LOG_PATH}\")\n",
    "    print(\"找到的最优超参数:\")\n",
    "    print(best_params)\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae65ba-4f8b-4cf0-8f0a-66e8151c6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.模型评估 (Model Evaluation)\n",
    "def train_and_evaluate_final_model(best_params):\n",
    "    print(\"--- 开始执行: 最终模型训练与评估 ---\")\n",
    "    train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    # 使用最优的预处理器\n",
    "    final_scaler = PowerTransformer()\n",
    "    final_encoder = ce.BackwardDifferenceEncoder(cols=CATEGORICAL_FEATURES)\n",
    "\n",
    "    x_train, y_train = preprocess_features(train_data, train_data, final_encoder, final_scaler)\n",
    "    x_test, y_test = preprocess_features(test_data, train_data, final_encoder, final_scaler)\n",
    "\n",
    "    # 确保整数参数类型正确\n",
    "    for name in ['depth', 'iterations', 'min_data_in_leaf']:\n",
    "        best_params[name] = int(best_params[name])\n",
    "\n",
    "    final_model = CatBoostRegressor(**best_params, random_state=10, verbose=False)\n",
    "    final_model.fit(x_train, y_train)\n",
    "\n",
    "    # 保存模型\n",
    "    final_model.save_model(SAVED_MODEL_PATH)\n",
    "    print(f\"最终模型已训练并保存至: {SAVED_MODEL_PATH}\")\n",
    "\n",
    "    # 在训练集和测试集上评估\n",
    "    y_train_pred = final_model.predict(x_train)\n",
    "    y_test_pred = final_model.predict(x_test)\n",
    "\n",
    "    print(\"\\n最终模型性能评估:\")\n",
    "    print(\n",
    "        f\"  训练集 R2: {r2_score(y_train, y_train_pred):.4f}, RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}, MAE: {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "    print(\n",
    "        f\"  测试集 R2: {r2_score(y_test, y_test_pred):.4f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}, MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "    return final_model, final_encoder, final_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e67f7d-0cdc-4898-8661-d92d1845c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.模型解释 (Model Interpretation)\n",
    "def run_model_interpretation(model, encoder, scaler):\n",
    "    print(\"--- 开始执行: 模型解释 ---\")\n",
    "    train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "    # 准备完整数据集用于解释\n",
    "    x_train, _ = preprocess_features(train_data, train_data, encoder, scaler)\n",
    "    x_test, _ = preprocess_features(test_data, train_data, encoder, scaler)\n",
    "    x_all = pd.concat([x_train, x_test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # SHAP分析\n",
    "    print(\"正在计算SHAP值...\")\n",
    "    explainer = shap.Explainer(model, x_train)\n",
    "    shap_values_all = explainer(x_all)\n",
    "\n",
    "    # 特征过滤\n",
    "    exclude_prefixes = [\"OSM_\", \"Salt_\"]\n",
    "    exceptions = [\"OSM Concentration\", \"Salt Concentration\"]\n",
    "\n",
    "    keep_idx = []\n",
    "    selected_features = []\n",
    "    for idx, col in enumerate(x_all.columns):\n",
    "        if any(col.startswith(prefix) for prefix in exclude_prefixes) and (col not in exceptions):\n",
    "            continue\n",
    "        else:\n",
    "            keep_idx.append(idx)\n",
    "            selected_features.append(col)\n",
    "\n",
    "    shap_values_filtered = shap_values_all.values[:, keep_idx]\n",
    "    x_all_filtered = x_all[selected_features]\n",
    "\n",
    "    print(\"SHAP分析完成，正在生成图表...\")\n",
    "\n",
    "    # SHAP摘要图 (点图)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values_filtered, x_all_filtered, plot_type='dot', show=False)\n",
    "    plt.title(\"SHAP Feature Summary Plot\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # SHAP重要性条形图\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values_filtered, x_all_filtered, plot_type='bar', show=False)\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # PDP & ICE 分析 (以 'Monomer A2 concentration' 为例)\n",
    "    print(\"\\n正在为 'Monomer A2 concentration' 计算PDP/ICE数据...\")\n",
    "\n",
    "    feature_to_plot = 'Monomer A2 concentration'\n",
    "    if feature_to_plot not in NUMERIC_FEATURES:\n",
    "        print(f\"特征 '{feature_to_plot}' 不在数值特征列表中，跳过PDP/ICE分析。\")\n",
    "        print(\"-\" * 30 + \"\\n\")\n",
    "        return\n",
    "\n",
    "    raw_all = pd.concat([train_data, test_data], axis=0)\n",
    "    raw_medians = train_data[NUMERIC_FEATURES].median()\n",
    "\n",
    "    subsample_size = min(200, len(x_all))\n",
    "    X_sample = x_all.sample(subsample_size, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    raw_min, raw_max = raw_all[feature_to_plot].min(), raw_all[feature_to_plot].max()\n",
    "    grid_points = np.linspace(raw_min, raw_max, 50)\n",
    "\n",
    "    grid_df = pd.DataFrame(np.repeat(raw_medians.values[np.newaxis, :], 50, axis=0), columns=NUMERIC_FEATURES)\n",
    "    grid_df[feature_to_plot] = grid_points\n",
    "\n",
    "    scaled_grid_array = scaler.transform(grid_df)\n",
    "    scaled_grid_vals = scaled_grid_array[:, NUMERIC_FEATURES.index(feature_to_plot)]\n",
    "\n",
    "    ice_curves = np.zeros((len(X_sample), 50))\n",
    "    X_sample.columns = X_sample.columns.astype(str)  # 确保列名为字符串\n",
    "    for i, val in enumerate(scaled_grid_vals):\n",
    "        X_modified = X_sample.copy()\n",
    "        X_modified[feature_to_plot] = val\n",
    "        ice_curves[:, i] = model.predict(X_modified)\n",
    "\n",
    "    # 中心化ICE和PDP曲线\n",
    "    ice_centered = ice_curves - ice_curves.mean(axis=1, keepdims=True)\n",
    "    pdp_centered = ice_centered.mean(axis=0)\n",
    "\n",
    "    print(\"PDP/ICE数据计算完成，正在生成图表...\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for curve in ice_centered:\n",
    "        ax.plot(grid_points, curve, color='lightblue', alpha=0.5)\n",
    "    ax.plot(grid_points, pdp_centered, color='red', linestyle='--', linewidth=2, label='Centered PDP')\n",
    "\n",
    "    ax.set_xlabel(feature_to_plot)\n",
    "    ax.set_ylabel(\"Centered Prediction\")\n",
    "    ax.set_title(f\"ICE and PDP for {feature_to_plot}\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55a1cb-ef61-4db8-9e71-aa6bc93160fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.主函数入口\n",
    "if __name__ == '__main__':\n",
    "    run_data_preprocessing()\n",
    "    # run_model_selection()\n",
    "\n",
    "    best_params = run_hyperparameter_optimization(\n",
    "        best_encoder_name='BackwardDifferenceEncoder',\n",
    "        best_scaler_name='PowerTransformer'\n",
    "    )\n",
    "\n",
    "    final_model, final_encoder, final_scaler = train_and_evaluate_final_model(best_params)\n",
    "\n",
    "    run_model_interpretation(final_model, final_encoder, final_scaler)\n",
    "\n",
    "    print(\"所有流程执行完毕\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
